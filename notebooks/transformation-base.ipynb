{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MOsHUjgdIrIW",
    "outputId": "f84a093e-147f-470e-aad9-80fb51193c8e"
   },
   "source": [
    "# Fine tuning T5 summarization\n",
    "\n",
    "Base on code from https://github.com/huggingface/notebooks/blob/master/examples/summarization.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are working on a local system.\n",
      "Files will be searched relative to \"..\".\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "ON_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if ON_COLAB:\n",
    "    GIT_ROOT = 'https://github.com/furyhawk/text_summarization/raw/master'\n",
    "    os.system(f'wget {GIT_ROOT}/notebooks/setup.py')\n",
    "\n",
    "%run -i setup.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"$BASE_DIR/settings.py\"\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "\n",
    "# to print output of all statements and not just the last\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# otherwise text between $ signs will be interpreted as formula and printed in italic\n",
    "pd.set_option('display.html.use_mathjax', False)\n",
    "\n",
    "# path to import blueprints packages\n",
    "sys.path.append(BASE_DIR + '/packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4cRE8IbIrIV"
   },
   "source": [
    "If you're opening this Notebook on colab, you will probably need to install 🤗 Transformers and 🤗 Datasets as well as other dependencies. Uncomment the following cell and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (1.12.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (4.11.3)\n",
      "Requirement already satisfied: rouge-score in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (0.0.4)\n",
      "Requirement already satisfied: nltk in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (3.6.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (from datasets) (21.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (from datasets) (1.21.2)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (from datasets) (3.7.4.post0)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (from datasets) (5.0.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (from datasets) (1.3.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: huggingface-hub<0.1.0,>=0.0.14 in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (from datasets) (0.0.19)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (from datasets) (2021.10.0)\n",
      "Requirement already satisfied: dill in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (from datasets) (2.26.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (from transformers) (2021.9.30)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (from transformers) (3.3.0)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (from rouge-score) (0.14.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: click in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (from requests>=2.19.0->datasets) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (from aiohttp->datasets) (5.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (from aiohttp->datasets) (1.7.0)\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (from aiohttp->datasets) (4.0.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (from aiohttp->datasets) (3.0.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (from pandas->datasets) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\furyx\\miniconda3\\envs\\text\\lib\\site-packages (from pandas->datasets) (2.8.2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "! pip install datasets transformers rouge-score nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're opening this notebook locally, make sure your environment has an install from the last version of those libraries.\n",
    "\n",
    "To be able to share your model with the community and generate results like the one shown in the picture below via the inference API, there are a few more steps to follow.\n",
    "\n",
    "First you have to store your authentication token from the Hugging Face website (sign up [here](https://huggingface.co/join) if you haven't already!) then execute the following cell and input your username and password:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "#notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you need to install Git-LFS. Uncomment the following instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!apt install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/jessevig/bertviz.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BBC News summary dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = f'../data/BBC News Summary'\n",
    "\n",
    "\n",
    "# root_path = f'/kaggle/input/bbc-news-summary/BBC News Summary'\n",
    "\n",
    "\n",
    "def loadDataset(root_path):\n",
    "\n",
    "    types_of_articles = ['business',\n",
    "                         'entertainment', 'politics', 'sport', 'tech']\n",
    "    df = pd.DataFrame(columns=['title', 'article', 'summary'])\n",
    "\n",
    "    for type_of_article in types_of_articles:\n",
    "        # type_of_article = 'business'  # entertainment, politices, sport, tech\n",
    "        num_of_article = len(os.listdir(\n",
    "            f\"{root_path}/News Articles/{type_of_article}\"))\n",
    "\n",
    "        print(f'\"Reading {type_of_article} articles\"')\n",
    "        dataframe = pd.DataFrame(columns=['title', 'article', 'summary'])\n",
    "\n",
    "        for i in tqdm(range(num_of_article)):\n",
    "            with open(f'{root_path}/News Articles/{type_of_article}/{(i+1):03d}.txt', 'r', encoding=\"utf8\", errors='ignore') as f:\n",
    "                article = f.read().partition(\"\\n\")\n",
    "            with open(f'{root_path}/Summaries/{type_of_article}/{(i+1):03d}.txt', 'r', encoding=\"utf8\", errors='ignore') as f:\n",
    "                summary = f.read()\n",
    "\n",
    "            dataframe.loc[i] = [article[0], article[2].replace(\n",
    "                '\\n', ' ').replace('\\r', ''), summary]\n",
    "\n",
    "        df = df.append(dataframe, ignore_index=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Reading business articles\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad9575d674644edb853d97cfc7e3c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/510 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Reading entertainment articles\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4b06c04514146bbbcdf3622ce9dcfa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/386 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Reading politics articles\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7194132a2dd3476192af638bfa09bdba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/417 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Reading sport articles\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe6c1c213906487fae793b5f2c80ba52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/511 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Reading tech articles\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7a67dc0cf54271b68eafcb248bc344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/401 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = loadDataset(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe tex2jax_ignore\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>article</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ad sales boost Time Warner profit</td>\n",
       "      <td>Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (£600m) for the three months to December, from $639m year-earlier.  The firm, which is now one of the biggest investors in Goo...</td>\n",
       "      <td>TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn.For the full-year, TimeWarner posted a profit of $3.36bn, up 27% from its 2003 performance, while revenues grew 6.4% to $42.09b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dollar gains on Greenspan speech</td>\n",
       "      <td>The dollar has hit its highest level against the euro in almost three months after the Federal Reserve head said the US trade deficit is set to stabilise.  And Alan Greenspan highlighted the US g...</td>\n",
       "      <td>The dollar has hit its highest level against the euro in almost three months after the Federal Reserve head said the US trade deficit is set to stabilise.China's currency remains pegged to the dol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yukos unit buyer faces loan claim</td>\n",
       "      <td>The owners of embattled Russian oil giant Yukos are to ask the buyer of its former production unit to pay back a $900m (£479m) loan.  State-owned Rosneft bought the Yugansk unit for $9.3bn in a s...</td>\n",
       "      <td>Yukos' owner Menatep Group says it will ask Rosneft to repay a loan that Yugansk had secured on its assets.State-owned Rosneft bought the Yugansk unit for $9.3bn in a sale forced by Russia to part...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High fuel prices hit BA's profits</td>\n",
       "      <td>British Airways has blamed high fuel prices for a 40% drop in profits.  Reporting its results for the three months to 31 December 2004, the airline made a pre-tax profit of £75m ($141m) compared ...</td>\n",
       "      <td>Rod Eddington, BA's chief executive, said the results were \"respectable\" in a third quarter when fuel costs rose by £106m or 47.3%.To help offset the increased price of aviation fuel, BA last year...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pernod takeover talk lifts Domecq</td>\n",
       "      <td>Shares in UK drinks and food firm Allied Domecq have risen on speculation that it could be the target of a takeover by France's Pernod Ricard.  Reports in the Wall Street Journal and the Financia...</td>\n",
       "      <td>Pernod has reduced the debt it took on to fund the Seagram purchase to just 1.8bn euros, while Allied has improved the performance of its fast-food chains.Shares in UK drinks and food firm Allied ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               title  \\\n",
       "0  Ad sales boost Time Warner profit   \n",
       "1   Dollar gains on Greenspan speech   \n",
       "2  Yukos unit buyer faces loan claim   \n",
       "3  High fuel prices hit BA's profits   \n",
       "4  Pernod takeover talk lifts Domecq   \n",
       "\n",
       "                                                                                                                                                                                                   article  \\\n",
       "0   Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (£600m) for the three months to December, from $639m year-earlier.  The firm, which is now one of the biggest investors in Goo...   \n",
       "1   The dollar has hit its highest level against the euro in almost three months after the Federal Reserve head said the US trade deficit is set to stabilise.  And Alan Greenspan highlighted the US g...   \n",
       "2   The owners of embattled Russian oil giant Yukos are to ask the buyer of its former production unit to pay back a $900m (£479m) loan.  State-owned Rosneft bought the Yugansk unit for $9.3bn in a s...   \n",
       "3   British Airways has blamed high fuel prices for a 40% drop in profits.  Reporting its results for the three months to 31 December 2004, the airline made a pre-tax profit of £75m ($141m) compared ...   \n",
       "4   Shares in UK drinks and food firm Allied Domecq have risen on speculation that it could be the target of a takeover by France's Pernod Ricard.  Reports in the Wall Street Journal and the Financia...   \n",
       "\n",
       "                                                                                                                                                                                                   summary  \n",
       "0  TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn.For the full-year, TimeWarner posted a profit of $3.36bn, up 27% from its 2003 performance, while revenues grew 6.4% to $42.09b...  \n",
       "1  The dollar has hit its highest level against the euro in almost three months after the Federal Reserve head said the US trade deficit is set to stabilise.China's currency remains pegged to the dol...  \n",
       "2  Yukos' owner Menatep Group says it will ask Rosneft to repay a loan that Yugansk had secured on its assets.State-owned Rosneft bought the Yugansk unit for $9.3bn in a sale forced by Russia to part...  \n",
       "3  Rod Eddington, BA's chief executive, said the results were \"respectable\" in a third quarter when fuel costs rose by £106m or 47.3%.To help offset the increased price of aviation fuel, BA last year...  \n",
       "4  Pernod has reduced the debt it took on to fund the Seagram purchase to just 1.8bn euros, while Allied has improved the performance of its fast-food chains.Shares in UK drinks and food firm Allied ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2225 entries, 0 to 2224\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   title    2225 non-null   object\n",
      " 1   article  2225 non-null   object\n",
      " 2   summary  2225 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 52.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('bbc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure your version of Transformers is at least 4.11.0 since the functionality was introduced in that version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.11.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFASsisvIrIb"
   },
   "source": [
    "You can find a script version of this notebook to fine-tune your model in a distributed fashion using multiple GPUs or TPUs [here](https://github.com/huggingface/transformers/tree/master/examples/seq2seq)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEJBSTyZIrIb"
   },
   "source": [
    "# Fine-tuning a model on a summarization task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTCFado4IrIc"
   },
   "source": [
    "In this notebook, we will see how to fine-tune one of the [🤗 Transformers](https://github.com/huggingface/transformers) model for a summarization task. We will use the [BBC dataset](https://www.kaggle.com/pariza/bbc-news-summary) which contains BBC News Summary.\n",
    "\n",
    "We will see how to easily load the dataset for this task using 🤗 Datasets and how to fine-tune a model on it using the `Trainer` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"t5-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RRkXuteIrIh"
   },
   "source": [
    "This notebook is built to run  with any model checkpoint from the [Model Hub](https://huggingface.co/models) as long as that model has a sequence-to-sequence version in the Transformers library. Here we picked the [`t5-small`](https://huggingface.co/t5-small) checkpoint. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whPRbBNbIrIl"
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7QYTpxXIrIl"
   },
   "source": [
    "We will use the [🤗 Datasets](https://github.com/huggingface/datasets) library to download the data and get the metric we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the functions `load_dataset` and `load_metric`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "IreSlFmlIrIm"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, load_metric\n",
    "raw_datasets = Dataset.from_pandas(df)  # Load from dataframe created earlier\n",
    "\n",
    "metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzfPtOMoIrIu"
   },
   "source": [
    "The `dataset` object itself is [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains one key for the training, validation and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "GWiVUF0jIrIv",
    "outputId": "35e3ea43-f397-4a54-c90c-f2cf8d36873e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'article', 'summary'],\n",
       "    num_rows: 2225\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetInfo(description='', citation='', homepage='', license='', features={'title': Value(dtype='string', id=None), 'article': Value(dtype='string', id=None), 'summary': Value(dtype='string', id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name=None, config_name=None, version=None, splits=None, download_checksums=None, download_size=None, post_processing_size=None, dataset_size=None, size_in_bytes=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets.info "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether to use news headline or summary as label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_datasets = raw_datasets.remove_columns(\"title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset in train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'article', 'summary'],\n",
       "        num_rows: 2002\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'article', 'summary'],\n",
       "        num_rows: 112\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['title', 'article', 'summary'],\n",
       "        num_rows: 111\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 90% train, 10% test + validation\n",
    "train_testvalid = raw_datasets.train_test_split(test_size=0.1)\n",
    "# Split the 10% test + validation in half test, half validation\n",
    "test_valid = train_testvalid[\"test\"].train_test_split(test_size=0.5)\n",
    "# gather everyone if you want to have a single DatasetDict\n",
    "raw_datasets = DatasetDict({\n",
    "    \"train\": train_testvalid[\"train\"],\n",
    "    \"test\": test_valid[\"test\"],\n",
    "    \"valid\": test_valid[\"train\"]})\n",
    "\n",
    "raw_datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3EtYfeHIrIz"
   },
   "source": [
    "To access an actual element, you need to select a split first, then give an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "X6HrpprwIrIz",
    "outputId": "d7670bc0-42e4-4c09-8a6a-5c018ded7d95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Monsanto fined $1.5m for bribery',\n",
       " 'article': ' The US agrochemical giant Monsanto has agreed to pay a $1.5m (£799,000) fine for bribing an Indonesian official.  Monsanto admitted one of its employees paid the senior official two years ago in a bid to avoid environmental impact studies being conducted on its cotton. In addition to the penalty, Monsanto also agreed to three years\\' close monitoring of its business practices by the American authorities. It said it accepted full responsibility for what it called improper activities.  A former senior manager at Monsanto directed an Indonesian consulting firm to give a $50,000 bribe to a high-level official in Indonesia\\'s environment ministry in 2002. The manager told the company to disguise an invoice for the bribe as \"consulting fees\".  Monsanto was facing stiff opposition from activists and farmers who were campaigning against its plans to introduce genetically-modified cotton in Indonesia. Despite the bribe, the official did not authorise the waiving of the environmental study requirement. Monsanto also has admitted to paying bribes to a number of other high-ranking officials between 1997 and 2002.  The chemicals-and-crops firm said it became aware of irregularities at a Jakarta-based subsidiary in 2001 and launched an internal investigation before informing the US Department of Justice and the Securities and Exchange Commission (SEC). Monsanto faced both criminal and civil charges from the Department of Justice and the SEC. \"Companies cannot bribe their way into favourable treatment by foreign officials,\" said Christopher Wray, assistant US attorney general. Monsanto has agreed to pay $1m to the Department of Justice, adopt internal compliance measures, and co-operate with continuing civil and criminal investigations. It is also paying $500,000 to the SEC to settle the bribe charge and other related violations. Monsanto said it accepted full responsibility for its employees\\' actions, adding that it had taken \"remedial actions to address the activities in Indonesia\" and had been \"fully co-operative\" throughout the investigative process. ',\n",
       " 'summary': \"Monsanto also has admitted to paying bribes to a number of other high-ranking officials between 1997 and 2002.A former senior manager at Monsanto directed an Indonesian consulting firm to give a $50,000 bribe to a high-level official in Indonesia's environment ministry in 2002.The US agrochemical giant Monsanto has agreed to pay a $1.5m (£799,000) fine for bribing an Indonesian official.Monsanto faced both criminal and civil charges from the Department of Justice and the SEC.Monsanto has agreed to pay $1m to the Department of Justice, adopt internal compliance measures, and co-operate with continuing civil and criminal investigations.Monsanto admitted one of its employees paid the senior official two years ago in a bid to avoid environmental impact studies being conducted on its cotton.\"}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHUmphG3IrI3"
   },
   "source": [
    "To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "i3j8APAoIrI3"
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=5):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "SZy5tRB_IrI7",
    "outputId": "ba8f2124-e485-488f-8c0c-254f34f24f13"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe tex2jax_ignore\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>article</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Web photo storage market hots up</td>\n",
       "      <td>An increasing number of firms are offering web storage for people with digital photo collections.  Digital cameras were the hot gadget of Christmas 2004 and worldwide sales of the cameras totalled $24bn last year. Many people's hard drives are bulging with photos and services which allow them to store and share their pictures online are becoming popular. Search firms such as Google are also offering more complex tools for managing personal photo libraries. Photo giants such as Kodak offer website storage which manages photo collections, lets users edit pictures online and provides print-ordering services. Some services, such as Kodak's Ofoto and Snapfish, offer unlimited storage space but they do require users to buy some prints online. Other sites, such as Pixagogo, charge a monthly fee. Marcus Hawkins, editor of Digital Camera magazine, said: \"As file sizes of pictures increase, storage becomes a problem. \"People are using their hard drives, backing up on CD and DVD and now they are using online storage solutions.  \"They are a place to store pictures, to share their pictures with families and friends and they can print out their photos.\"  While many of the services are aimed at the amateur and casual digital photographer, other websites are geared up for enthusiasts who want to share tips and information. Photosig is an online community of photographers who can critique each other's work. On Tuesday, Google released free software for organising and finding digital photos stored on a computer's hard drive. The tool, called Picasa, automatically detects photos as they are added to a PC - whether sent via e-mail or transferred from a digital camera.  The software includes tools for restoring colour and removing red eye, as well as sharpening images. Photos can then be uploaded to sites such as Ofoto. Many people use the sites to edit and improve their favourite photographs before ordering prints. Mr Hawkins added: \"The growth area is that you can order your prints online. Friends and family can also access pictures you want them to see and they can print them out too. \"Rather than just a place to dump your pictures, it's about sharing them.\"  The vast majority of pictures remain on a PC's hard drive, which is why search tools, such as those offered by Google, become increasingly important. But some historians and archivists are concerned that the need for perfect pictures will mean that those poor quality prints which offered a tantilising glimpse of the past may disappear forever. \"It's one thing taking pictures, it's another finding them,\" said Mr Hawkins. \"But this is the same problem that has always existed - how many of us have photos in wallets tucked away somewhere?\"</td>\n",
       "      <td>Many people's hard drives are bulging with photos and services which allow them to store and share their pictures online are becoming popular.Photo giants such as Kodak offer website storage which manages photo collections, lets users edit pictures online and provides print-ordering services.\"They are a place to store pictures, to share their pictures with families and friends and they can print out their photos.\"An increasing number of firms are offering web storage for people with digital photo collections.Marcus Hawkins, editor of Digital Camera magazine, said: \"As file sizes of pictures increase, storage becomes a problem.On Tuesday, Google released free software for organising and finding digital photos stored on a computer's hard drive.Some services, such as Kodak's Ofoto and Snapfish, offer unlimited storage space but they do require users to buy some prints online.Mr Hawkins added: \"The growth area is that you can order your prints online.The tool, called Picasa, automatically detects photos as they are added to a PC - whether sent via e-mail or transferred from a digital camera.Friends and family can also access pictures you want them to see and they can print them out too.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tough rules for ringtone sellers</td>\n",
       "      <td>Firms that flout rules on how ringtones and other mobile extras are sold could be cut off from all UK phone networks.  The rules allow offenders to be cut off if they do not let consumers know exactly what they get for their money and how to turn off the services. The first month under the new rules has seen at least ten firms suspended while they clean up the way they work. The rules have been brought in to ensure that the problems plaguing net users do not spread to mobile phones.  In the last couple of years ringtones, wallpapers, screensavers and lots of other extras for phones have become hugely popular. But fierce competition is making it difficult for firms to get their wares in front of consumers, said Jeremy Flynn, head of third party services at Vodafone.  \"If you are not on the operator's portal you are going to have quite heavy marketing costs because it's a problem of how people discover your services,\" he said. To combat this many ringtone and other mobile content sellers started using a new tactic to squeeze more cash out of customers. This tactic involved signing people up for a subscription to give them, for instance, several ringtones per week or month instead of the single track they thought they were getting. Mr Flynn said that the move to using subscriptions happened over the space of a few weeks at the end of 2004. Websites such as grumbletext.co.uk started getting reports from customers who were racking up large bills for phone content they did not know they had signed up for. \"What made us uncomfortable was that these services were not being marketed transparently,\" said Mr Flynn. \"People did not know they were being offered a subscription service.\" \"We saw potential for substantial consumer harm here,\" he added.  The swift adoption of subscription services led to the creation of a new code of conduct for firms that want to sell content for mobile phones. The drafting of the new rules was led by the Mobile Entertainment Forum and the UK's phone firms. \"Everyone is required to conform to this code of conduct,\" said Andrew Bud, regulatory head of the MEF and executive chairman of messaging firm MBlox.  \"It's all about transparency,\" he said. \"Consumers have to be told what they have got themselves into and how to get out of it.\" \"The consumer has a right to be protected,\" he said. Christian Harris, partnership manager of mobile content firm Zed, said the new system was essential if consumers were to trust companies that sell ringtones and other downloads. \"The groundrules must be applied across the whole industry and if that's done effectively we will see the cowboys driven out,\" he said. The new rules came in to force on 15 January and the first month under the new regime has seen many firms cautioned for not honouring them. Some have been told to revamp websites so customers know what they get for their money and what they are signing up for, said Mr Flynn. Also, said Mr Flynn, Vodafone has briefly cut off between eight and ten content sellers flouting the rules. \"We have quite draconian contracts with firms,\" he said. \"We do not have to say why. We can just cut them off.\" Under the rules consumers must be able to switch off the services by using a universal \"stop\" command sent via text message. He said the system had been designed to limit how much a consumer will pay if they inadvertently signed up for a service. \"The mobile is so personal that people really resent the abuse of what is effectively part of their personality,\" said Mr Flynn.</td>\n",
       "      <td>Christian Harris, partnership manager of mobile content firm Zed, said the new system was essential if consumers were to trust companies that sell ringtones and other downloads.Some have been told to revamp websites so customers know what they get for their money and what they are signing up for, said Mr Flynn.Also, said Mr Flynn, Vodafone has briefly cut off between eight and ten content sellers flouting the rules.\"The consumer has a right to be protected,\" he said.\"The mobile is so personal that people really resent the abuse of what is effectively part of their personality,\" said Mr Flynn.\"What made us uncomfortable was that these services were not being marketed transparently,\" said Mr Flynn.\"It's all about transparency,\" he said.Mr Flynn said that the move to using subscriptions happened over the space of a few weeks at the end of 2004.But fierce competition is making it difficult for firms to get their wares in front of consumers, said Jeremy Flynn, head of third party services at Vodafone.He said the system had been designed to limit how much a consumer will pay if they inadvertently signed up for a service.The swift adoption of subscription services led to the creation of a new code of conduct for firms that want to sell content for mobile phones.Firms that flout rules on how ringtones and other mobile extras are sold could be cut off from all UK phone networks.The rules allow offenders to be cut off if they do not let consumers know exactly what they get for their money and how to turn off the services.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mourinho defiant on Chelsea form</td>\n",
       "      <td>Chelsea boss Jose Mourinho has insisted that Sir Alex Ferguson and Arsene Wenger would swap places with him.  Mourinho's side were knocked out of the FA Cup by Newcastle last Sunday before seeing Barcelona secure a 2-1 Champions League first-leg lead in the Nou Camp. But he denied his club was suffering a dip in form which league rivals Arsenal and Manchester United could exploit. \"They cannot speak to us about blips because they're not in a better position than us,\" Mourinho said. \"Do they want to change positions with us? We are top of the league by nine points and in the Carling Cup final. \"The only thing they can say they are in a better position than us in is the FA Cup. \"In the Champions League all three teams can either go through or go out but the one team that is in the best position is still Chelsea.\"  Mourinho said it was important to keep his team's results in perspective. \"Don't try to put pressure on me because I am never under pressure,\" he warned. \"We have lost one important game this week - at Newcastle - and we're out of the FA Cup but I don't think a defeat in a first-leg tie is a real defeat. We are just 2-1 down at half-time.\" Asked if his Chelsea honeymoon was now over, Mourinho replied: \"I have had 20 years of honeymoons with my wife. \"The day that this club is not happy with me is the day that I go.\"</td>\n",
       "      <td>\"The only thing they can say they are in a better position than us in is the FA Cup.\"They cannot speak to us about blips because they're not in a better position than us,\" Mourinho said.\"In the Champions League all three teams can either go through or go out but the one team that is in the best position is still Chelsea.\"\"We have lost one important game this week - at Newcastle - and we're out of the FA Cup but I don't think a defeat in a first-leg tie is a real defeat.Mourinho's side were knocked out of the FA Cup by Newcastle last Sunday before seeing Barcelona secure a 2-1 Champions League first-leg lead in the Nou Camp.Mourinho said it was important to keep his team's results in perspective.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Old Firm pair handed suspensions</td>\n",
       "      <td>Celtic's Henri Camara and Nacho Novo of Rangers have both been suspended for offences missed by the referee in a recent Old Firm game.  Both were given automatic one-match bans and 12 additional disciplinary points for their actions. That means Novo will miss a further two games because of his prior record. Camara will miss one additional game. Novo was found guilty of stamping on Celtic's Stephen Pearson. Camara was punished for kicking Gregory Vignal. \"I'm extremely disappointed. I don't know if there's a lot to add to that,\" said Rangers manager Alex McLeish of Novo's punishment. \"But what I will say is that I'm at a loss as to why that incident should be picked up and highlighted when there were so many other incidents during the game.\" Both players will miss this weekend's games when Celtic host Dundee United and Rangers visit Kilmarnock unless they decide to appeal.  The additional bans come into effect from 28 December meaning Camara will also be unavailable for the home game against Livingston while Novo will miss the meetings with Dundee United and Dunfermline. An SFA spokesman said: \"They have seven days to appeal but with matches coming along at the weekend they would need to do so before Friday. \"But if they do appeal it won't be heard before this weekend because it takes a bit of time to seat up an appeals tribunal.\" Meanwhile, Bob Malcolm and Rangers have been informed by letter as to the outcome of the hearing regarding his reaction to Rangers being awarded a penalty in the same game. Malcolm, a substitute on the day, was taken from the Rangers dug-out and spoken to by police about an alleged gesture he made. But the SFA would not detail what, if any, punishment Malcolm would receive. \"Once Rangers receive our letter we will be in a position to make a comment on the findings,\" added the spokesman.</td>\n",
       "      <td>Camara will miss one additional game.Both players will miss this weekend's games when Celtic host Dundee United and Rangers visit Kilmarnock unless they decide to appeal.The additional bans come into effect from 28 December meaning Camara will also be unavailable for the home game against Livingston while Novo will miss the meetings with Dundee United and Dunfermline.Celtic's Henri Camara and Nacho Novo of Rangers have both been suspended for offences missed by the referee in a recent Old Firm game.Meanwhile, Bob Malcolm and Rangers have been informed by letter as to the outcome of the hearing regarding his reaction to Rangers being awarded a penalty in the same game.That means Novo will miss a further two games because of his prior record.Malcolm, a substitute on the day, was taken from the Rangers dug-out and spoken to by police about an alleged gesture he made.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brookside actress Keaveney dies</td>\n",
       "      <td>Actress Anna Keaveney, who appeared in Brookside, My Family and A Touch of Frost, has died of lung cancer aged 55.  Keaveney, who played Brookside's Marie Jackson when the Channel 4 soap began in 1982, died on Saturday. Born in Runcorn, Cheshire, she also starred in numerous films including 1989's Shirley Valentine. She played Nellie in Mike Leigh's latest film Vera Drake, which won the Golden Lion prize for best film at this year's Venice Film Festival.  And most recently she appeared alongside Richard Wilson in ITV's King of Fridges and with Martin Clunes in Doc Martin. Other TV appearances included the part of Matron in ITV drama Footballers' Wives and a cameo role as Tom Farrell's mother Sheila in BBC comedy Gimme Gimme Gimme. Keaveney's career also included stage performances in Neaptide for the National Theatre, Private Lives and The Rise and Fall of Little Voice. The actress died in hospital. Her agent Barry Brown said: \"Anna was due to have had another operation on Friday but unfortunately she was too weak.\"</td>\n",
       "      <td>Actress Anna Keaveney, who appeared in Brookside, My Family and A Touch of Frost, has died of lung cancer aged 55.The actress died in hospital.Keaveney, who played Brookside's Marie Jackson when the Channel 4 soap began in 1982, died on Saturday.Born in Runcorn, Cheshire, she also starred in numerous films including 1989's Shirley Valentine.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnjDIuQ3IrI-"
   },
   "source": [
    "The metric is an instance of [`datasets.Metric`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Metric):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "5o4rUteaIrI_",
    "outputId": "18038ef5-554c-45c5-e00a-133b02ec10f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metric(name: \"rouge\", features: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}, usage: \"\"\"\n",
       "Calculates average rouge scores for a list of hypotheses and references\n",
       "Args:\n",
       "    predictions: list of predictions to score. Each predictions\n",
       "        should be a string with tokens separated by spaces.\n",
       "    references: list of reference for each prediction. Each\n",
       "        reference should be a string with tokens separated by spaces.\n",
       "    rouge_types: A list of rouge types to calculate.\n",
       "        Valid names:\n",
       "        `\"rouge{n}\"` (e.g. `\"rouge1\"`, `\"rouge2\"`) where: {n} is the n-gram based scoring,\n",
       "        `\"rougeL\"`: Longest common subsequence based scoring.\n",
       "        `\"rougeLSum\"`: rougeLsum splits text using `\"\n",
       "\"`.\n",
       "        See details in https://github.com/huggingface/datasets/issues/617\n",
       "    use_stemmer: Bool indicating whether Porter stemmer should be used to strip word suffixes.\n",
       "    use_agregator: Return aggregates if this is set to True\n",
       "Returns:\n",
       "    rouge1: rouge_1 (precision, recall, f1),\n",
       "    rouge2: rouge_2 (precision, recall, f1),\n",
       "    rougeL: rouge_l (precision, recall, f1),\n",
       "    rougeLsum: rouge_lsum (precision, recall, f1)\n",
       "Examples:\n",
       "\n",
       "    >>> rouge = datasets.load_metric('rouge')\n",
       "    >>> predictions = [\"hello there\", \"general kenobi\"]\n",
       "    >>> references = [\"hello there\", \"general kenobi\"]\n",
       "    >>> results = rouge.compute(predictions=predictions, references=references)\n",
       "    >>> print(list(results.keys()))\n",
       "    ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
       "    >>> print(results[\"rouge1\"])\n",
       "    AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))\n",
       "    >>> print(results[\"rouge1\"].mid.fmeasure)\n",
       "    1.0\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAWdqcUBIrJC"
   },
   "source": [
    "You can call its `compute` method with your predictions and labels, which need to be list of decoded strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "6XN1Rq0aIrJC",
    "outputId": "a4405435-a8a9-41ff-9f79-a13077b587c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
       " 'rouge2': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
       " 'rougeL': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
       " 'rougeLsum': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_preds = [\"hello there\", \"general kenobi\"]\n",
    "fake_labels = [\"hello there\", \"general kenobi\"]\n",
    "metric.compute(predictions=fake_preds, references=fake_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9qywopnIrJH"
   },
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVx71GdAIrJH"
   },
   "source": [
    "Before we can feed those texts to our model, we need to preprocess them. This is done by a 🤗 Transformers `Tokenizer` which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that the model requires.\n",
    "\n",
    "To do all of this, we instantiate our tokenizer with the `AutoTokenizer.from_pretrained` method, which will ensure:\n",
    "\n",
    "- we get a tokenizer that corresponds to the model architecture we want to use,\n",
    "- we download the vocabulary used when pretraining this specific checkpoint.\n",
    "\n",
    "That vocabulary will be cached, so it's not downloaded again the next time we run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "eXNLu_-nIrJI"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint) # t5-base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vl6IidfdIrJK"
   },
   "source": [
    "By default, the call above will use one of the fast tokenizers (backed by Rust) from the 🤗 Tokenizers library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vl6IidfdIrJK"
   },
   "source": [
    "Load some required javascript libraries for displaying visualization in notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rowT4iCLIrJK"
   },
   "source": [
    "You can directly call this tokenizer on one sentence or a pair of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "a5hBlsrHIrJL",
    "outputId": "acdaa98a-a8cd-4a20-89b8-cc26437bbe90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [8774, 6, 48, 80, 7142, 55, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello, this one sentence!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare the targets for our model, we need to tokenize them inside the `as_target_tokenizer` context manager. This will make sure the tokenizer uses the special tokens corresponding to the targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[8774, 6, 48, 80, 7142, 55, 1], [100, 19, 430, 7142, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "with tokenizer.as_target_tokenizer():\n",
    "    print(tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁He', '▁didn', 't', '▁want', '▁to', '▁talk', '▁about', '▁cells', '▁on', '▁the', '▁cell', '▁phone', '▁because', '▁', 'he', '▁considered', '▁it', '▁boring', '</s>']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"He didnt want to talk about cells on the cell phone because he considered it boring\"\n",
    "inputs = tokenizer.encode(sentence, return_tensors='pt', add_special_tokens=True) # return PyTorch tensors\n",
    "tokens = tokenizer.convert_ids_to_tokens(list(inputs[0])) # Extract sample of batch index 0 from inputs list of lists\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2C0hcmp9IrJQ"
   },
   "source": [
    "If you are using one of the five T5 checkpoints we have to prefix the inputs with \"summarize:\" (the model can also translate and it needs the prefix to know which task it has to perform)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
    "    prefix = \"summarize: \"\n",
    "else:\n",
    "    prefix = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2C0hcmp9IrJQ"
   },
   "source": [
    "We can then write the function that will preprocess our samples. We just feed them to the `tokenizer` with the argument `truncation=True`. This will ensure that an input longer that what the model selected can handle will be truncated to the maximum length accepted by the model. The padding will be dealt with later on (in a data collator) so we pad examples to the longest length in the batch and not the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "vc0BSBLIIrJQ"
   },
   "outputs": [],
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # x var for summarization. Column article.\n",
    "    inputs = [prefix + doc for doc in examples[\"article\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets y. We are using column summary as label.\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lm8ozrJIrJR"
   },
   "source": [
    "This function works with one or several examples. In the case of several examples, the tokenizer will return a list of lists for each key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "-b70jh26IrJS",
    "outputId": "acd3a42d-985b-44ee-9daa-af5d944ce1d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[21603, 10, 37, 837, 3, 9, 3844, 14676, 6079, 2963, 7, 288, 32, 65, 4686, 12, 726, 3, 9, 1514, 16593, 51, 41, 19853, 4440, 23938, 61, 1399, 21, 3, 2160, 115, 53, 46, 9995, 29, 2314, 5, 2963, 7, 288, 32, 10246, 80, 13, 165, 1652, 1866, 8, 2991, 2314, 192, 203, 977, 16, 3, 9, 6894, 12, 1792, 3262, 1113, 2116, 271, 4468, 30, 165, 7282, 5, 86, 811, 12, 8, 10736, 6, 2963, 7, 288, 32, 92, 4686, 12, 386, 203, 31, 885, 4891, 13, 165, 268, 2869, 57, 8, 797, 5779, 5, 94, 243, 34, 4307, 423, 3263, 21, 125, 34, 718, 22187, 1087, 5, 71, 1798, 2991, 2743, 44, 2963, 7, 288, 32, 6640, 46, 9995, 29, 9157, 1669, 12, 428, 3, 9, 29788, 3, 2160, 346, 12, 3, 9, 306, 18, 4563, 2314, 16, 9995, 31, 7, 1164, 8409, 16, 4407, 5, 37, 2743, 1219, 8, 349, 12, 31993, 46, 10921, 21, 8, 3, 2160, 346, 38, 96, 29492, 53, 3051, 1280, 2963, 7, 288, 32, 47, 5008, 14537, 8263, 45, 19053, 11, 7208, 113, 130, 2066, 53, 581, 165, 1390, 12, 4277, 6472, 1427, 18, 7360, 3676, 7282, 16, 9995, 5, 3, 4868, 8, 3, 2160, 346, 6, 8, 2314, 410, 59, 2291, 159, 15, 8, 8036, 23, 3745, 13, 8, 3262, 810, 5971, 5, 2963, 7, 288, 32, 92, 65, 10246, 12, 3788, 3, 2160, 346, 7, 12, 3, 9, 381, 13, 119, 306, 18, 6254, 53, 4298, 344, 6622, 11, 4407, 5, 37, 9356, 18, 232, 18, 2771, 102, 7, 1669, 243, 34, 1632, 2718, 13, 22085, 2197, 44, 3, 9, 31711, 18, 390, 20438, 16, 4402, 11, 3759, 46, 3224, 4962, 274, 16, 10454, 8, 837, 1775, 13, 6923, 11, 8, 20571, 11, 8231, 3527, 41, 134, 3073, 137, 2963, 7, 288, 32, 7865, 321, 4336, 11, 3095, 3991, 45, 8, 1775, 13, 6923, 11, 8, 180, 3073, 5, 96, 5890, 2837, 725, 1178, 3, 2160, 346, 70, 194, 139, 3, 30786, 1058, 57, 2959, 4298, 976, 243, 14702, 549, 2866, 6, 6165, 837, 4917, 879, 5, 2963, 7, 288, 32, 65, 4686, 12, 726, 1970, 51, 12, 8, 1775, 13, 6923, 6, 4693, 3224, 5856, 3629, 6, 11, 576, 18, 18140, 17, 15, 28, 6168, 3095, 11, 4336, 17032, 5, 94, 19, 92, 3788, 1514, 23221, 12, 8, 180, 3073, 12, 8955, 8, 3, 2160, 346, 1567, 11, 119, 1341, 17880, 5, 2963, 7, 288, 32, 243, 34, 4307, 423, 3263, 21, 165, 1652, 31, 2874, 6, 2651, 24, 34, 141, 1026, 96, 60, 8172, 40, 2874, 12, 1115, 8, 1087, 16, 9995, 121, 11, 141, 118, 96, 5195, 576, 18, 11480, 121, 1019, 8, 20184, 3268, 433, 5, 1], [21603, 10, 7190, 31, 7, 9637, 5072, 49, 3, 7, 20978, 326, 46, 13423, 9883, 12, 1369, 8, 1640, 51, 44, 1771, 31, 7, 19931, 1331, 1338, 5, 5072, 49, 3, 75, 11863, 4357, 4834, 3978, 12, 4081, 8, 1338, 1368, 11, 2369, 168, 2177, 13, 3434, 31, 7, 10290, 10689, 526, 6, 113, 14602, 8, 689, 16, 4357, 3708, 4220, 7, 5, 37, 296, 5297, 6336, 243, 10, 96, 196, 530, 12, 8, 3761, 11, 82, 9883, 47, 13423, 11, 27, 47, 29335, 5, 27, 966, 877, 234, 5, 96, 196, 1800, 3, 9, 385, 394, 1771, 1379, 68, 1500, 27, 31, 26, 163, 661, 16, 8, 711, 1964, 5, 37, 29, 762, 877, 3923, 535, 5072, 49, 6, 294, 13, 8, 1651, 7190, 314, 226, 2915, 51, 31395, 24, 751, 2045, 44, 8, 486, 3225, 7, 17793, 6, 56, 230, 919, 112, 1388, 12, 416, 1851, 31, 7, 30894, 3545, 1611, 25483, 10570, 16, 23826, 5, 96, 517, 757, 29, 27, 183, 341, 326, 18, 24814, 27, 214, 132, 19, 2500, 72, 16, 8, 5040, 11, 27, 1672, 12, 129, 3627, 16, 8, 416, 360, 1274, 976, 3, 88, 243, 5, 96, 196, 17, 31, 7, 131, 3, 9, 495, 13, 6591, 2462, 550, 38, 27, 43, 612, 16, 1767, 203, 11, 8, 772, 56, 369, 535, 8288, 31, 7, 15498, 2143, 11390, 47, 92, 16, 1041, 16, 19931, 5, 216, 3, 14417, 323, 45, 112, 14499, 15, 26, 4837, 51, 12, 2382, 51, 12, 1992, 1025, 16, 1401, 5, 5865, 4220, 7, 5, 3434, 31, 7, 10135, 1793, 7, 35, 19855, 751, 8, 1964, 16, 1401, 5, 4560, 4220, 7, 28, 10098, 348, 8643, 4049, 4011, 4524, 511, 16, 1401, 5, 3449, 4220, 7, 5, 290, 130, 2500, 13, 119, 2991, 2390, 9227, 2924, 70, 5297, 607, 147, 8, 1851, 5, 749, 51, 4890, 1640, 51, 23463, 52, 3, 75, 11863, 3, 9, 126, 1270, 1368, 13, 4306, 3916, 3978, 44, 3, 9, 1338, 16, 16491, 5, 37, 12371, 1201, 18, 1490, 3495, 8, 3946, 16, 160, 1678, 68, 141, 12, 8955, 21, 4494, 166, 286, 28, 1798, 22656, 6336, 23561, 21329, 9423, 16, 8, 804, 5, 3, 6, 113, 8238, 2400, 8, 1038, 3112, 44, 8, 11548, 5880, 336, 774, 6, 356, 46, 5297, 525, 200, 13, 10128, 1752, 51, 16, 8, 12063, 4418, 44, 3, 9, 1338, 16, 350, 107, 295, 5, 466, 14527, 3, 18, 6862, 75, 51, 710, 13, 18065, 4668, 446, 15311, 3, 26705, 23, 32, 31, 7, 1941, 3, 18, 47, 207, 631, 12, 9448, 21, 8, 1611, 25483, 7666, 7, 5, 486, 8, 337, 1338, 6, 2369, 1025, 16, 4306, 2555, 3978, 16, 3, 9, 306, 18, 4057, 887, 31, 7, 1640, 51, 5, 37, 605, 47, 751, 57, 1611, 9365, 3960, 21110, 3, 28150, 106, 13, 1410, 298, 15575, 8374, 6777, 961, 900, 49, 17, 47, 511, 5, 7190, 31, 7, 2194, 867, 5428, 76, 5667, 2369, 8486, 16, 4306, 2469, 5, 11548, 13467, 3, 107, 6707, 21774, 9365, 3350, 263, 3, 9, 731, 18, 4397, 1205, 12, 1041, 44, 46, 5297, 1338, 16, 15922, 5, 37, 2059, 18, 1201, 18, 1490, 13139, 1300, 3959, 51, 12, 1369, 8, 306, 4418, 11, 3, 17, 13296, 8808, 3840, 51, 16, 8, 887, 31, 7, 2538, 474, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[2963, 7, 288, 32, 92, 65, 10246, 12, 3788, 3, 2160, 346, 7, 12, 3, 9, 381, 13, 119, 306, 18, 6254, 53, 4298, 344, 6622, 11, 4407, 5, 188, 1798, 2991, 2743, 44, 2963, 7, 288, 32, 6640, 46, 9995, 29, 9157, 1669, 12, 428, 3, 9, 29788, 3, 2160, 346, 12, 3, 9, 306, 18, 4563, 2314, 16, 9995, 31, 7, 1164, 8409, 16, 4407, 5, 634, 837, 3, 9, 3844, 14676, 6079, 2963, 7, 288, 32, 65, 4686, 12, 726, 3, 9, 1514, 16593, 51, 41, 19853, 4440, 23938, 61, 1399, 21, 3, 2160, 115, 53, 46, 9995, 29, 2314, 5, 9168, 7, 288, 32, 7865, 321, 4336, 11, 3095, 3991, 45, 8, 1775, 13, 6923, 11, 8, 180, 3073, 5, 9168, 7, 288, 1], [486, 8, 337, 1338, 6, 2369, 1025, 16, 4306, 2555, 3978, 16, 3, 9, 306, 18, 4057, 887, 31, 7, 1640, 51, 5, 21846, 537, 49, 3, 75, 11863, 4357, 4834, 3978, 12, 4081, 8, 1338, 1368, 11, 2369, 168, 2177, 13, 3434, 31, 7, 10290, 10689, 526, 6, 113, 14602, 8, 689, 16, 4357, 3708, 4220, 7, 5, 634, 296, 5297, 6336, 243, 10, 96, 196, 530, 12, 8, 3761, 11, 82, 9883, 47, 13423, 11, 27, 47, 29335, 5, 279, 10694, 77, 31, 7, 9637, 5072, 49, 3, 7, 20978, 326, 46, 13423, 9883, 12, 1369, 8, 1640, 51, 44, 1771, 31, 7, 19931, 1331, 1338, 5, 667, 120, 51, 6174, 13467, 3, 107, 6707, 21774, 9365, 3350, 263, 3, 9, 731, 18, 4397, 1205, 1]]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_function(raw_datasets['train'][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zS-6iXTkIrJT"
   },
   "source": [
    "To apply this function on all the pairs of sentences in our dataset, we just use the `map` method of our `dataset` object we created earlier. This will apply the function on all the elements of all the splits in `dataset`, so our training, validation and testing data will be preprocessed in one single command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "DDtsaJeVIrJT",
    "outputId": "aa4734bf-4ef5-4437-9948-2c16363da719"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 2002, 'test': 112, 'valid': 111}\n"
     ]
    }
   ],
   "source": [
    "# raw_datasets['train'] = raw_datasets['train'].shard(num_shards=100, index=3)\n",
    "# raw_datasets['validation'] = raw_datasets['validation'].shard(num_shards=100, index=3)\n",
    "# raw_datasets['test'] = raw_datasets['test'].shard(num_shards=100, index=3)\n",
    "print(raw_datasets.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b208465f78c54f01a0226570df259f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9383926da104bc3843c2ee6cf80712a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07c07626c59f4490846877b6145ce961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voWiw8C7IrJV"
   },
   "source": [
    "Even better, the results are automatically cached by the 🤗 Datasets library to avoid spending time on this step the next time you run your notebook. The 🤗 Datasets library is normally smart enough to detect when the function you pass to map has changed (and thus requires to not use the cache data). For instance, it will properly detect if you change the task in the first cell and rerun the notebook. 🤗 Datasets warns you when it uses cached files, you can pass `load_from_cache_file=False` in the call to `map` to not use the cached files and force the preprocessing to be applied again.\n",
    "\n",
    "Note that we passed `batched=True` to encode the texts by batches together. This is to leverage the full benefit of the fast tokenizer we loaded earlier, which will use multi-threading to treat the texts in a batch concurrently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "545PP3o8IrJV"
   },
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBiW8UpKIrJW"
   },
   "source": [
    "Now that our data is ready, we can download the pretrained model and fine-tune it. Since our task is of the sequence-to-sequence kind, we use the `AutoModelForSeq2SeqLM` class. Like with the tokenizer, the `from_pretrained` method will download and cache the model for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "TlqNaB8jIrJW",
    "outputId": "84916cf3-6e6c-47f3-d081-032ec30a4132"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CczA5lJlIrJX"
   },
   "source": [
    "Note that  we don't get a warning like in our classification example. This means we used all the weights of the pretrained model and there is no randomly initialized head in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N8urzhyIrJY"
   },
   "source": [
    "To instantiate a `Seq2SeqTrainer`, we will need to define three more things. The most important is the [`Seq2SeqTrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.Seq2SeqTrainingArguments), which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{model_name}-finetuned-bbc\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "km3pGVdTIrJc"
   },
   "source": [
    "Here we set the evaluation to be done at the end of each epoch, tweak the learning rate, use the `batch_size` defined at the top of the cell and customize the weight decay. Since the `Seq2SeqTrainer` will save the model regularly and our dataset is quite large, we tell it to make three saves maximum. Lastly, we use the `predict_with_generate` option (to properly generate summaries) and activate mixed precision training (to go a bit faster).\n",
    "\n",
    "The last argument to setup everything so we can push the model to the [Hub](https://huggingface.co/models) regularly during training. Remove it if you didn't follow the installation steps at the top of the notebook. If you want to save your model locally in a name that is different than the name of the repository it will be pushed, or if you want to push your model under an organization and not your name space, use the `hub_model_id` argument to set the repo name (it needs to be the full name, including your namespace: for instance `\"sgugger/t5-finetuned-xsum\"` or `\"huggingface/t5-finetuned-xsum\"`).\n",
    "\n",
    "Then, we need a special kind of data collator, which will not only pad the inputs to the maximum length in the batch, but also the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sZOdRlRIrJd"
   },
   "source": [
    "The last thing to define for our `Seq2SeqTrainer` is how to compute the metrics from the predictions. We need to define a function for this, which will just use the `metric` we loaded earlier, and we have to do a bit of pre-processing to decode the predictions into texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "UmvbnJ9JIrJd"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXuFTAzDIrJe"
   },
   "source": [
    "Then we just need to pass all of this along with our datasets to the `Seq2SeqTrainer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "imY1oC3SIrJf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated git hooks.\n",
      "Git LFS initialized.\n"
     ]
    }
   ],
   "source": [
    "!git lfs install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "imY1oC3SIrJf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W:\\workspace\\text_summarization\\notebooks\\t5-base-finetuned-bbc is already a clone of https://huggingface.co/furyhawk/t5-base-finetuned-bbc. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdzABDVcIrJg"
   },
   "source": [
    "We can now finetune our model by just calling the `train` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\furyx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: summary, title, article.\n",
      "***** Running training *****\n",
      "  Num examples = 2002\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 167\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='167' max='167' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [167/167 7:03:48, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.260809</td>\n",
       "      <td>20.619600</td>\n",
       "      <td>15.252500</td>\n",
       "      <td>19.093400</td>\n",
       "      <td>19.271600</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: summary, title, article.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 111\n",
      "  Batch size = 12\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=167, training_loss=0.8271075951125093, metrics={'train_runtime': 25534.2229, 'train_samples_per_second': 0.078, 'train_steps_per_second': 0.007, 'total_flos': 2474367178752000.0, 'train_loss': 0.8271075951125093, 'epoch': 1.0})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now upload the result of the training to the Hub, just execute this instruction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to t5-base-finetuned-bbc\n",
      "Configuration saved in t5-base-finetuned-bbc\\config.json\n",
      "Model weights saved in t5-base-finetuned-bbc\\pytorch_model.bin\n",
      "tokenizer config file saved in t5-base-finetuned-bbc\\tokenizer_config.json\n",
      "Special tokens file saved in t5-base-finetuned-bbc\\special_tokens_map.json\n",
      "Copy vocab file to t5-base-finetuned-bbc\\spiece.model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d6d947458b48bcb484df39b640073e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/850M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed338a56f50140089fef7830feb849ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Oct29_17-29-30_ALIENMEDIA/1635499922.574325/events.out.tfevents.1635499922.ALIENMEDIA.13000.1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "274fcc7f3a4946e98dcd1399717328ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Oct29_17-29-30_ALIENMEDIA/events.out.tfevents.1635499922.ALIENMEDIA.13000.0: 100%|##########|…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c1996baf69e4b458d33ebc99286ed48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Oct29_17-37-24_ALIENMEDIA/1635500279.469721/events.out.tfevents.1635500279.ALIENMEDIA.13000.5…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a4445188004dac94c0ec752b1a6c3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file spiece.model:   4%|4         | 32.0k/773k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "400d50840f754e42b6b0de70d9e4543a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Oct29_17-29-30_ALIENMEDIA/1635500206.5130718/events.out.tfevents.1635500206.ALIENMEDIA.13000.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db5d2ff442dd4cf9b606a68674905ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Oct29_18-17-58_ALIENMEDIA/1635502690.8740766/events.out.tfevents.1635502690.ALIENMEDIA.5128.1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a9a50ebc3b4adda3e4451ef9fcf6e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Oct29_17-40-51_ALIENMEDIA/1635500472.3932552/events.out.tfevents.1635500472.ALIENMEDIA.13000.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dd37594d39f4bc9a056f14c533af79b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Oct29_17-37-24_ALIENMEDIA/events.out.tfevents.1635500279.ALIENMEDIA.13000.4: 100%|##########|…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ca1631153d4e00a3eb8776497fe7d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Oct29_17-29-30_ALIENMEDIA/1635500129.2624795/events.out.tfevents.1635500129.ALIENMEDIA.13000.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a7b3a51e56472989ea1829ee84b59b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Oct29_18-17-58_ALIENMEDIA/events.out.tfevents.1635502690.ALIENMEDIA.5128.0: 100%|##########| …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c4b8154265b4610af4f04e4154420b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Oct29_17-40-51_ALIENMEDIA/events.out.tfevents.1635500472.ALIENMEDIA.13000.6: 100%|##########|…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab6636116c34ea3b1c874f13fe1b61a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file training_args.bin: 100%|##########| 2.86k/2.86k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/furyhawk/t5-base-finetuned-bbc\n",
      "   519baa1..a93787d  main -> main\n",
      "\n",
      "Dropping the following result as it does not have all the necessary field:\n",
      "{'task': {'name': 'Sequence-to-sequence Language Modeling', 'type': 'text2text-generation'}}\n",
      "To https://huggingface.co/furyhawk/t5-base-finetuned-bbc\n",
      "   a93787d..f587d43  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/furyhawk/t5-base-finetuned-bbc/commit/a93787d3dd6bb3f018e3c338cfdbab5f3692e7c3'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now share this model with all your friends, family, favorite pets: they can all load it with the identifier `\"your-username/the-name-you-picked\"` so for instance:\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"sgugger/my-awesome-model\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Summarization",
   "provenance": []
  },
  "interpreter": {
   "hash": "9a7204dd9f60bd662761bdbb6f90c481306084606f9758a59a17813579c978f3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
